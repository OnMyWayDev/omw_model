# -*- coding: utf-8 -*-
"""OMW_gemma.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nf5qXO3bEenR5kNgBzrc2ZR-sw1qjPgs
"""

-----------------------------------------------------------------------------------------------------

# 다운로드 받아놓을 패키지 및 모델

!pip3 install -q -U transformers==4.38.2
!pip3 install -q -U bitsandbytes==0.42.0
!pip3 install -q -U peft==0.9.0
!pip3 install -q -U trl==0.7.11
!pip3 install -q -U accelerate==0.27.2

#tokenizer 파일 받아서 경로 상에 놓기

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

FINETUNE_MODEL = "/content/drive/MyDrive/Kgma/content_2/drive_2/MyDrive_2/OnMyWay/kogma"

finetune_model = AutoModelForCausalLM.from_pretrained(FINETUNE_MODEL, device_map={"":0})
tokenizer = "/content/drive/MyDrive/Tokenizer"

# document text는 줄 띄우기 없이 모두 이어지게
doc = "요약하고 싶은 파일 경로"

pipe_finetuned = pipeline("text-generation", model=finetune_model, tokenizer=tokenizer, max_new_tokens=512)

messages = [ { "role": "user", "content": "다음 글을 요약해주세요:\n\n{}".format(doc)} ]
prompt = pipe_finetuned.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

outputs = pipe_finetuned(
    prompt,
    do_sample=True,
    temperature=0.2,
    top_k=50,
    top_p=0.95,
    add_special_tokens=True )
print(outputs[0]["generated_text"][len(prompt):])

